{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "Classifing student success data by means of the [MLPClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) from the sklearn module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data/ Data Transformation\n",
    "For fitting a multi-layer perception model for classification it is suggested that you standardize the design matrix otherwise optimization of the weights are less likely to converge in fewer iterations. Hence after importing the data and converting the response class labels, we scale the design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\006988889-SA\\Downloads\\conda\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.extmath import cartesian\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "df = pd.read_csv('student-por2.csv')\n",
    "df = pd.get_dummies(df)#, drop_first=True)\n",
    "\n",
    "def response_conv(arr):\n",
    "    new = []\n",
    "    for i in arr:\n",
    "        if (i > 0 and i < 10):           # condition where student failed\n",
    "            new.append(0)                 \n",
    "                                          \n",
    "        elif (i >= 10):                   # condition where student passed\n",
    "            new.append(1)                 \n",
    "    \n",
    "        else:                             # condition where student received an incomplete\n",
    "            new.append(2)\n",
    "    return(new)                           # 1-dimensional response varibale returned\n",
    "\n",
    "X = df.drop('G3',1)                       # This is the design matrix\n",
    "y = list(df.G3)                           # This is the discrete response vector\n",
    "y_new = response_conv(y)                  # This is the multinomial response vector\n",
    "\n",
    "clf = MLPClassifier(learning_rate_init=1)\n",
    "clf.fit(X,y)\n",
    "\n",
    "select = SelectPercentile(percentile=50)\n",
    "newX = select.fit_transform(X,y)\n",
    "\n",
    "X_scale = preprocessing.scale(newX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Parameters for NN\n",
    "Here we use 10-fold cross validation to make sure that we return the model with the best accuracy. We also print out the values for each parameter that make the model optimal. Lastly return the run time of the cross validation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters for standardized design matrix are as follows: \n",
      "Hidden layer size: 30 \n",
      "Activation function: relu \n",
      "Solver for weight optimization: sgd \n",
      "Learning rate: constant \n",
      "Alpha(penalty parameter): 0.1\n",
      "Cross Validation took 0.38333333333333336 minutes.\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_scale, y_new, test_size=0.33, random_state=42)\n",
    "\n",
    "start_time = time.time()\n",
    "combos = cartesian([['constant'],['sgd'],['logistic', 'tanh', 'relu'],10.0**-np.arange(1,7)])\n",
    "in_layer_size = len(list(X))\n",
    "out_layer_size = 3\n",
    "hidden_layer_size = int((in_layer_size+out_layer_size)/2)\n",
    "\n",
    "def opt(X,y):\n",
    "    acc = []\n",
    "    for learn,solver,act,alpha in combos:\n",
    "        nn = MLPClassifier(hidden_layer_sizes=(hidden_layer_size,),activation=act,solver=solver,learning_rate=learn,alpha=float(alpha),learning_rate_init = 0.5,random_state=42)\n",
    "        scores = cross_val_score(nn, X, y, cv=10, scoring='accuracy')\n",
    "        acc.append(scores.mean())\n",
    "    \n",
    "\n",
    "    opt_ = combos[acc.index(max(acc))]\n",
    "    return(opt_)\n",
    "\n",
    "lea1,sol1,act1,alp1 = opt(X1_train,y1_train)\n",
    "\n",
    "print(\"Optimal parameters for standardized design matrix are as follows: \\nHidden layer size: %r \\nActivation function: %s \\nSolver for weight optimization: %s \\nLearning rate: %s \\nAlpha(penalty parameter): %r\" % (hidden_layer_size,act1,sol1,lea1,float(alp1)))\n",
    "print(\"Cross Validation took %r minutes.\" % (int(time.time() - start_time)/60))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and Plot the Cost Function for Stochastic Gradient Descent\n",
    "Here we fit the model with the optimal parameters found in the last section. We return a plot of the Cost Function converging to its minimum after a specific number of iterations. As expected we see convergance and have a good model for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0xc234eb8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIjEKSmIiEickoKIiMT9P4Rc0oaTxGezAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x4d31278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp1 = MLPClassifier(hidden_layer_sizes=(hidden_layer_size,),activation=act1,solver=sol1,learning_rate=lea1,learning_rate_init = 0.5,alpha=float(alp1),random_state=42)\n",
    "nn1 = mlp1.fit(X1_train,y1_train)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"SGD Cost Function\")\n",
    "plt.xlabel(\"Number of Iterations for SGD\")\n",
    "plt.ylabel(\"Cost Function\")\n",
    "plt.plot(nn1.loss_curve_, 'r-', label='Standardized Matrix')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Layers and Weighted Coefficients\n",
    "This step is to get an idea of the weighted coefficients used for each neuron in the *hidden* and *outside* layers. The hidden layer is denoted by *0* and the outside layer is denoted by *1*. **Note** In this example we only have one hidden layer and one outside layer. In previous steps we found the best number of neurons on the hidden layer was 30, thus in the hidden layer the first neuron is denoted by *0* and the last neuron is denoted by *29*. For each neuron in every layer the *SGD* optimal weights array is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer, Neuron, and Weights for standardized design matrix.\n",
      "Layer: 0 \n",
      "Neuron: 0 \n",
      "Weights: array([ 0.01873731, -0.15733035, -0.28000846, -0.039092  , -0.36589339,\n",
      "        0.28648056, -0.08493128,  0.00873698,  0.27525273,  0.17100422,\n",
      "        0.00503006,  0.19074378, -0.10744227,  0.38565959, -0.24637327,\n",
      "       -0.03608078,  0.10848505,  0.26853964,  0.13845995,  0.00631909,\n",
      "       -0.30422643,  0.35668506, -0.18657706,  0.06085536,  0.09961485,\n",
      "       -0.3138226 ,  0.28013505,  0.29487564, -0.29524329]), \n",
      "Layer: 0 \n",
      "Neuron: 1 \n",
      "Weights: array([ 0.04689559,  0.08205119,  0.14155282,  0.12834731,  0.20007118,\n",
      "       -0.56746094,  0.08335715, -0.26860239, -0.36373771, -0.00924034,\n",
      "       -0.25389543,  0.05473831,  0.15931461, -0.24948385,  0.28550131,\n",
      "        0.08561244, -0.04034874, -0.02965284,  0.17592524,  0.3216026 ,\n",
      "        0.08599517, -0.21722489,  0.07255007,  0.13905398, -0.41315344,\n",
      "       -0.0540447 ,  0.13998625, -0.04824451,  0.13772642]), \n",
      "Layer: 0 \n",
      "Neuron: 2 \n",
      "Weights: array([-0.35407026, -0.20178457,  0.52020356,  0.04734304,  0.41665199,\n",
      "       -0.37983416,  0.46644871,  0.27397938,  0.51744842, -0.36187976,\n",
      "        0.22984465, -0.58236333, -0.24558702,  0.17044777, -0.26604556,\n",
      "        0.158221  , -0.17981285,  0.39332145, -0.26779974, -0.45546732,\n",
      "       -0.26499794,  0.12977155,  0.30146774, -0.09394305, -0.00506658,\n",
      "       -0.00316233, -0.01829701, -0.06764492, -0.05654189]), \n",
      "Layer: 0 \n",
      "Neuron: 3 \n",
      "Weights: array([ 0.15736649, -0.05443021, -0.11928559, -0.08171939, -0.07720452,\n",
      "       -0.10735729, -0.00942331, -0.32170959, -0.21671538,  0.11156703,\n",
      "       -0.01185585, -0.0816053 ,  0.10189658,  0.2852836 , -0.23704919,\n",
      "        0.00352061,  0.09156723,  0.34441507, -0.06276822,  0.08951561,\n",
      "       -0.13082537, -0.00755688, -0.02322131,  0.01866862, -0.03095158,\n",
      "       -0.11615247,  0.05926879,  0.03783165,  0.11923367]), \n",
      "Layer: 0 \n",
      "Neuron: 4 \n",
      "Weights: array([ 0.04937205,  0.16732658, -0.09290264, -0.01373445, -0.28128177,\n",
      "        0.33841462, -0.06714779,  0.07176818, -0.24980999,  0.20566146,\n",
      "        0.32277353, -0.18255722, -0.10451767,  0.00228419, -0.10943365,\n",
      "       -0.57879934,  0.52457112, -0.31793187,  0.09859934, -0.02186542,\n",
      "        0.10279008, -0.31863629,  0.10930233, -0.15680352,  0.20820006,\n",
      "       -0.0475119 , -0.0734387 ,  0.04482388, -0.04151857]), \n",
      "Layer: 0 \n",
      "Neuron: 5 \n",
      "Weights: array([ 0.04450276,  0.10499649,  0.10995086,  0.03667669,  0.03917095,\n",
      "       -0.08452237, -0.02690187, -0.02248831, -0.00102471, -0.00795904,\n",
      "       -0.05551961, -0.1020243 ,  0.0641512 ,  0.03079362,  0.05474359,\n",
      "        0.03992268,  0.03020865, -0.01941434,  0.03174535,  0.20700792,\n",
      "        0.00058375, -0.038394  , -0.00947921,  0.00042979, -0.07804071,\n",
      "       -0.01157631,  0.13668466,  0.01097265, -0.02282279]), \n",
      "Layer: 0 \n",
      "Neuron: 6 \n",
      "Weights: array([ 0.12805668, -0.02561591, -0.00331448,  0.06955886, -0.00975572,\n",
      "       -0.07274615,  0.09212053,  0.17165811,  0.09800098,  0.08488595,\n",
      "        0.03085804,  0.2886005 ,  0.29764671, -0.04067983,  0.09409862,\n",
      "       -0.04420707, -0.12377338, -0.05276325,  0.04576618,  0.00816496,\n",
      "        0.02450861, -0.0546163 , -0.01872143, -0.08839741, -0.00402809,\n",
      "        0.04241963, -0.00618334, -0.18620074,  0.12860887]), \n",
      "Layer: 0 \n",
      "Neuron: 7 \n",
      "Weights: array([ 0.32729865, -0.28266528,  0.16061574,  0.19184172,  0.32624437,\n",
      "       -0.25764117,  0.18625689,  0.31265869,  0.05819017,  0.07819491,\n",
      "       -0.22754079,  0.62803913,  0.68333089, -0.03247107,  0.08887386,\n",
      "       -0.02482472, -0.01976636, -0.08169462, -0.12928008, -0.14684257,\n",
      "       -0.43271844, -0.03041195,  0.07973017,  0.38176599, -0.16555507,\n",
      "        0.2438542 , -0.2555529 , -0.01438763,  0.08489186]), \n",
      "Layer: 0 \n",
      "Neuron: 8 \n",
      "Weights: array([-0.38313527, -0.022318  , -0.26563914, -0.37699249, -0.0910797 ,\n",
      "       -0.30580095, -0.43835142, -0.00621242, -0.15294458,  0.16908448,\n",
      "        0.24936836, -0.28148732, -0.17722301, -0.18102779,  0.19915551,\n",
      "       -0.20303018,  0.20422606,  0.08634793, -0.04730803,  0.03448848,\n",
      "       -0.14009682,  0.1439023 ,  0.13645715, -0.22649113,  0.12764675,\n",
      "        0.20589011, -0.14250592,  0.0711599 , -0.15006429]), \n",
      "Layer: 0 \n",
      "Neuron: 9 \n",
      "Weights: array([ 0.14190725, -0.23520447, -0.0535228 , -0.0807821 , -0.05468257,\n",
      "        0.04539696, -0.28021721, -0.19329232,  0.06597661,  0.06704301,\n",
      "       -0.16112846, -0.21176094, -0.06430851,  0.46130924, -0.48417505,\n",
      "       -0.1413944 ,  0.14301182,  0.08137573, -0.05738725,  0.09688946,\n",
      "        0.00963449, -0.12461363, -0.16083288,  0.22284055, -0.20906404,\n",
      "       -0.08413686,  0.21519448, -0.07297875,  0.06061571]), \n",
      "Layer: 0 \n",
      "Neuron: 10 \n",
      "Weights: array([-0.12876501, -0.4014762 , -0.0875088 ,  0.00255266, -0.0950487 ,\n",
      "        0.13985333, -0.27149645,  0.09937509,  0.13164186,  0.21685464,\n",
      "        0.09033945,  0.23363529,  0.0524601 ,  0.18943247, -0.32369652,\n",
      "       -0.08349197,  0.00747743,  0.23396631, -0.01993678,  0.00315088,\n",
      "       -0.25459307, -0.03682191, -0.02849485,  0.14055368, -0.06896665,\n",
      "       -0.11498627, -0.00878599, -0.01732333, -0.00584167]), \n",
      "Layer: 0 \n",
      "Neuron: 11 \n",
      "Weights: array([ 0.07929023,  0.20543839, -0.12791285,  0.08251068, -0.28908565,\n",
      "        0.23583945, -0.18740533, -0.26530238, -0.25610564,  0.19963683,\n",
      "       -0.03593893, -0.03882975, -0.47455073,  0.03215558,  0.0861117 ,\n",
      "        0.05753663, -0.00956765, -0.13805685,  0.12004287,  0.177582  ,\n",
      "       -0.03233277, -0.07353316, -0.00671568,  0.12383671,  0.01693181,\n",
      "       -0.0911933 ,  0.07301891, -0.21483029,  0.17112411]), \n",
      "Layer: 0 \n",
      "Neuron: 12 \n",
      "Weights: array([-0.00909321, -0.05578382, -0.00691338, -0.0370657 , -0.04658673,\n",
      "       -0.25171193,  0.03399433, -0.09501151,  0.00277417, -0.04203388,\n",
      "       -0.15844081,  0.27438059,  0.28451865,  0.16363932, -0.14164969,\n",
      "       -0.03406504,  0.03823714,  0.03282185,  0.01831885,  0.02483946,\n",
      "        0.02058021, -0.04603831, -0.11012259, -0.0051023 ,  0.01476151,\n",
      "       -0.16910879,  0.12536438, -0.06426948, -0.01704845]), \n",
      "Layer: 0 \n",
      "Neuron: 13 \n",
      "Weights: array([-0.04604607,  0.04489969,  0.00932007,  0.01297604,  0.0079159 ,\n",
      "        0.05304839, -0.01981164,  0.00943837, -0.00500595, -0.16870402,\n",
      "        0.06691838, -0.0808928 ,  0.04624759, -0.00388987, -0.00900784,\n",
      "        0.03887847, -0.07957527,  0.10813036,  0.00105512, -0.05428271,\n",
      "       -0.1436659 , -0.04631564,  0.15028739, -0.07702247, -0.01678991,\n",
      "        0.03408555,  0.02920997,  0.03320531,  0.01955091]), \n",
      "Layer: 0 \n",
      "Neuron: 14 \n",
      "Weights: array([-0.13000957,  0.10512491,  0.13987839, -0.03582934,  0.30584813,\n",
      "       -0.29120178,  0.01873858, -0.13019521, -0.1986529 ,  0.01069864,\n",
      "        0.03905808,  0.09250898,  0.18845698,  0.12524879, -0.23136634,\n",
      "        0.06170255,  0.0141435 ,  0.04500935, -0.04673663,  0.07933555,\n",
      "        0.01087177, -0.0159151 , -0.0358418 , -0.04861263, -0.03544711,\n",
      "       -0.24654268,  0.07764372, -0.02043003, -0.02542349]), \n",
      "Layer: 0 \n",
      "Neuron: 15 \n",
      "Weights: array([ 0.01056839, -0.06308152, -0.10266201,  0.33344242, -0.30637751,\n",
      "       -0.14801109, -0.02767813, -0.20090235,  0.28671046, -0.007714  ,\n",
      "       -0.5387414 , -0.2316815 , -0.80824985,  0.04567837,  0.06055472,\n",
      "        0.35309927, -0.40150179,  0.12007365, -0.06765996,  0.07567813,\n",
      "       -0.02170939, -0.05838046,  0.25842589,  0.20209673,  0.15176203,\n",
      "       -0.02980469,  0.10852568,  0.11176489, -0.09207753]), \n",
      "Layer: 0 \n",
      "Neuron: 16 \n",
      "Weights: array([-0.09565049,  0.21858433,  0.06569723, -0.16608057, -0.21490746,\n",
      "       -0.25335591,  0.34075111, -0.04687717, -0.00792902,  0.04423581,\n",
      "       -0.24610847,  0.31460071,  0.42880498, -0.1311679 ,  0.03534579,\n",
      "        0.21009117, -0.23335622, -0.4362587 , -0.02257167, -0.06482421,\n",
      "       -0.08934449,  0.1054575 , -0.08247682, -0.09821206,  0.13291588,\n",
      "       -0.01433562,  0.13803722,  0.26333587, -0.37323989]), \n",
      "Layer: 0 \n",
      "Neuron: 17 \n",
      "Weights: array([  7.18588522e-02,   7.25583372e-02,  -2.55069719e-01,\n",
      "        -5.13031301e-01,   7.92586752e-02,  -6.75576400e-02,\n",
      "         1.88723073e-01,   4.25661045e-01,  -1.83577179e-01,\n",
      "         8.31252267e-02,  -1.51098734e-01,  -1.05542179e+00,\n",
      "        -1.83058180e+00,  -2.82073627e-02,  -1.58855934e-03,\n",
      "        -1.24004182e-01,   1.14446423e-01,  -5.90718100e-02,\n",
      "         3.42434528e-02,  -1.65485530e-01,  -2.29112812e-01,\n",
      "         9.08666667e-02,   3.14107763e-01,  -2.96858666e-02,\n",
      "         6.68414751e-02,  -2.91738243e-02,   1.07874378e-01,\n",
      "        -1.70619822e-02,  -1.51698637e-02]), \n",
      "Layer: 0 \n",
      "Neuron: 18 \n",
      "Weights: array([ 0.01054421, -0.18170373, -0.12167299,  0.09240327,  0.04234285,\n",
      "       -0.10693335,  0.13578722, -0.16199141, -0.21405586, -0.12888027,\n",
      "       -0.14464202,  0.05689862,  0.15150706,  0.08759489,  0.02133313,\n",
      "        0.04709783, -0.01183642,  0.00246193,  0.02573064, -0.06508254,\n",
      "        0.0526347 ,  0.03779793,  0.11283329, -0.04966295,  0.08905696,\n",
      "        0.02060404, -0.01370591,  0.00690433,  0.04694174]), \n",
      "Layer: 0 \n",
      "Neuron: 19 \n",
      "Weights: array([ 0.23459157, -0.02463353, -0.03554838, -0.07917166, -0.03693998,\n",
      "       -0.18677707,  0.15591745, -0.16297783, -0.16187755,  0.01207765,\n",
      "       -0.46463699,  0.12441785, -0.26249133, -0.26585195,  0.11850137,\n",
      "        0.0607292 ,  0.10836138, -0.02371546,  0.24261571, -0.01145522,\n",
      "       -0.11279114, -0.16455663,  0.4005981 ,  0.13279374, -0.20775071,\n",
      "        0.1524842 , -0.02002387, -0.03494034, -0.05091516]), \n",
      "Layer: 0 \n",
      "Neuron: 20 \n",
      "Weights: array([-0.05773699, -0.15981914, -0.06513555, -0.12659528, -0.07039058,\n",
      "       -0.21585613,  0.02912467, -0.54516016, -0.10353276, -0.178942  ,\n",
      "        0.14223889,  0.35358377,  0.18120139,  0.22853707, -0.33465552,\n",
      "       -0.47984372,  0.4282521 ,  0.37191217, -0.05570232, -0.23360096,\n",
      "        0.15522037, -0.20542169, -0.00590277, -0.0257345 ,  0.08320742,\n",
      "       -0.18242236,  0.14998575, -0.34019866,  0.34022225]), \n",
      "Layer: 0 \n",
      "Neuron: 21 \n",
      "Weights: array([ -5.52588979e-02,   3.42315411e-03,   1.08060662e-01,\n",
      "        -1.43532164e-03,  -8.66209769e-02,  -2.97225967e-02,\n",
      "        -2.03744544e-01,  -1.30471259e-01,  -2.75544727e-01,\n",
      "         5.63551422e-02,  -1.37047647e-02,   3.93272832e-01,\n",
      "         3.21437458e-01,   1.02236786e-01,   6.45863120e-02,\n",
      "         6.87904496e-02,   2.13218704e-02,   3.56596551e-01,\n",
      "        -9.98105635e-02,   1.58019259e-04,  -2.20917506e-01,\n",
      "        -5.23343324e-02,   3.71024855e-01,  -3.44227194e-02,\n",
      "         1.36020765e-02,  -2.92919177e-02,   2.91122781e-02,\n",
      "        -1.42496053e-01,   1.25706619e-01]), \n",
      "Layer: 0 \n",
      "Neuron: 22 \n",
      "Weights: array([-0.00400355,  0.11213336, -0.05319035,  0.03325122,  0.04187139,\n",
      "       -0.03840087,  0.13038113, -0.02232858,  0.09309156, -0.0384678 ,\n",
      "       -0.09381806,  0.35491101,  0.22629953,  0.31133154, -0.25347198,\n",
      "       -0.0648877 , -0.05039627, -0.10680268,  0.08380035,  0.04248045,\n",
      "       -0.20694479,  0.04108629,  0.05013621,  0.10670755,  0.15385482,\n",
      "        0.04105633, -0.09381256, -0.21423437,  0.15461615]), \n",
      "Layer: 0 \n",
      "Neuron: 23 \n",
      "Weights: array([ 0.03994491,  0.17096772, -0.14442753,  0.24782643,  0.37886346,\n",
      "       -0.36710691,  0.18480853, -0.11119548, -0.39562461, -0.45235993,\n",
      "        0.00530105,  0.16534279,  0.33217158, -0.0786062 ,  0.06864706,\n",
      "       -0.08058167,  0.20612856, -0.16997496, -0.11133433, -0.05351534,\n",
      "        0.02774585,  0.06458502,  0.09242821, -0.12068028,  0.16146789,\n",
      "        0.02339762, -0.02268482, -0.26886829,  0.20068406]), \n",
      "Layer: 0 \n",
      "Neuron: 24 \n",
      "Weights: array([-0.3258267 , -0.10236597,  0.05751512, -0.07262885,  0.01647746,\n",
      "        0.23660216, -0.06860133, -0.03233981,  0.20548747, -0.37697215,\n",
      "        0.52062942, -0.20693822,  0.25946562, -0.2034696 ,  0.28265778,\n",
      "       -0.01207875,  0.04991216, -0.11714839, -0.18839118, -0.08154576,\n",
      "        0.12330406,  0.13567097,  0.13490198, -0.34362904,  0.06346202,\n",
      "       -0.04395131,  0.08338647, -0.18942187,  0.17145744]), \n",
      "Layer: 0 \n",
      "Neuron: 25 \n",
      "Weights: array([ 0.23733964,  0.56951981,  0.38016774,  0.19137689, -0.18120541,\n",
      "       -0.45361954, -0.21231286,  0.26029551,  0.25965292,  0.1580779 ,\n",
      "        0.16871071,  0.56440119,  0.45009626,  0.03483394,  0.03563772,\n",
      "        0.18354138, -0.15583285,  0.05637409,  0.32534021, -0.24243472,\n",
      "        0.03283063, -0.21668551, -0.01187048,  0.17845592,  0.10409176,\n",
      "       -0.22790433,  0.20766576, -0.17514972,  0.17133236]), \n",
      "Layer: 0 \n",
      "Neuron: 26 \n",
      "Weights: array([-0.14942335, -0.04753383,  0.11312744,  0.11857166,  0.12753804,\n",
      "       -0.02251006, -0.12572607, -0.0654069 ,  0.09245117,  0.11539616,\n",
      "        0.00448699,  0.03236094,  0.06865409, -0.05956591,  0.10929251,\n",
      "       -0.16645491,  0.16373426,  0.04842572, -0.02386454, -0.00361082,\n",
      "        0.00388604, -0.0639863 ,  0.14737036, -0.02991044, -0.01555099,\n",
      "       -0.10187124,  0.13101108,  0.16157678, -0.14909469]), \n",
      "Layer: 0 \n",
      "Neuron: 27 \n",
      "Weights: array([ 0.07713014, -0.57398158, -0.20363468,  0.26658077, -0.10451567,\n",
      "        0.26755279, -0.14587461, -0.57999086, -0.13741909,  0.71638025,\n",
      "        0.15571432, -0.35495085, -0.04860555,  0.15076339, -0.13615188,\n",
      "        0.03495338,  0.03208057, -0.05225504, -0.04660602, -0.09397121,\n",
      "        0.14823526,  0.0821725 , -0.29336326,  0.04257342,  0.09477804,\n",
      "       -0.05452265,  0.1698972 , -0.21281244,  0.13165787]), \n",
      "Layer: 0 \n",
      "Neuron: 28 \n",
      "Weights: array([ 0.00576249, -0.50646127, -0.0597924 ,  0.11509239, -0.09527077,\n",
      "        0.17884768, -0.03516766,  0.19332638, -0.08824304,  0.08916845,\n",
      "       -0.22867228,  0.2324672 ,  0.18159612,  0.11041455, -0.10172736,\n",
      "        0.03536109,  0.12832768,  0.08685369,  0.04487092, -0.08361267,\n",
      "       -0.26717017,  0.22376289,  0.18230901, -0.14826394,  0.15970682,\n",
      "        0.02332139,  0.00711098,  0.06025577,  0.01852941]), \n",
      "Layer: 0 \n",
      "Neuron: 29 \n",
      "Weights: array([-0.00048437, -0.02030344,  0.02772324, -0.04597582, -0.02018105,\n",
      "       -0.02253075,  0.01564743,  0.02463056, -0.00483903, -0.00141831,\n",
      "       -0.04199701,  0.02844252,  0.06022887,  0.01313863, -0.04236865,\n",
      "        0.01207412,  0.03223934, -0.05428964,  0.02128723, -0.03771983,\n",
      "        0.01116959, -0.04109438, -0.00184706, -0.01139427, -0.00491788,\n",
      "       -0.03266596, -0.01107704, -0.01568092,  0.11868822]), \n",
      "\n",
      "Layer: 1 \n",
      "Neuron: 0 \n",
      "Weights: array([-0.88132819, -0.6608169 ,  1.0984383 , -0.08346766,  0.82182163,\n",
      "       -0.11940203,  0.08791924, -0.57511476,  0.61744347, -0.09351546,\n",
      "       -0.29958093,  0.37617631,  0.00991073,  0.10799504,  0.01202831,\n",
      "        0.12228059, -0.48187807,  0.32187388, -0.17954995, -0.67078007,\n",
      "       -0.62369361, -0.31542272, -0.18844795, -0.60699831,  0.16721618,\n",
      "       -0.53072247,  0.18732382,  0.81410819, -0.3306415 , -0.18178663]), \n",
      "Layer: 1 \n",
      "Neuron: 1 \n",
      "Weights: array([ 0.60340661,  0.57853064, -0.7422306 ,  0.45488929, -0.45007452,\n",
      "        0.16766191,  0.21145688,  1.0019791 , -0.41964534,  0.63820317,\n",
      "        0.52081837, -0.70036333, -0.03436567,  0.05926172,  0.38593347,\n",
      "       -0.98455877,  0.84694021, -1.53728687,  0.28814122,  0.17039606,\n",
      "        0.8213255 ,  0.59340635,  0.45147404,  0.63769332,  0.47857884,\n",
      "        0.79357135,  0.08168934, -0.46337338,  0.6099776 ,  0.3095182 ]), \n",
      "Layer: 1 \n",
      "Neuron: 2 \n",
      "Weights: array([ 0.019748  ,  0.17685884, -0.37557184, -0.43380097, -0.35136088,\n",
      "       -0.00562278, -0.23455878, -0.2644944 , -0.38441052, -0.56064275,\n",
      "       -0.11200746,  0.19789828, -0.08260108, -0.18993923, -0.35329152,\n",
      "        0.75461718, -0.22889154,  1.27609464, -0.15125876,  0.36971505,\n",
      "       -0.33305952, -0.38903521, -0.24336271,  0.10660227, -0.76993605,\n",
      "       -0.40364221, -0.28628918, -0.48107865, -0.18120467, -0.12765258]), \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Layer, Neuron, and Weights for standardized design matrix.\")\n",
    "for i in range(len(mlp1.coefs_)):\n",
    "    n_neurons_per_layer = mlp1.coefs_[i].shape[1]\n",
    "    for j in range(n_neurons_per_layer):\n",
    "        w = mlp1.coefs_[i][:,j]\n",
    "        print(\"Layer: %r \\nNeuron: %r \\nWeights: %r\" %(i, j, w) , end=\", \")\n",
    "        print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "This step is to make predictions on the test set we set aside that the model has yet to see and was not trianed on. Returns a data frame of the actaul *y* values and the perdicted *y* values. This data frame helps us to visually see where our model made incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_act</th>\n",
       "      <th>y_nn_stan</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Obs</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>215 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     y_act  y_nn_stan\n",
       "Obs                  \n",
       "0        1          1\n",
       "1        1          1\n",
       "2        1          1\n",
       "3        1          1\n",
       "4        1          1\n",
       "5        1          1\n",
       "6        1          1\n",
       "7        0          0\n",
       "8        1          1\n",
       "9        1          1\n",
       "10       1          1\n",
       "11       1          1\n",
       "12       1          1\n",
       "13       0          1\n",
       "14       1          1\n",
       "15       1          1\n",
       "16       1          1\n",
       "17       0          0\n",
       "18       1          1\n",
       "19       1          1\n",
       "20       1          1\n",
       "21       1          1\n",
       "22       1          1\n",
       "23       1          1\n",
       "24       1          1\n",
       "25       1          1\n",
       "26       0          1\n",
       "27       1          1\n",
       "28       1          1\n",
       "29       1          1\n",
       "..     ...        ...\n",
       "185      1          1\n",
       "186      1          1\n",
       "187      0          0\n",
       "188      1          1\n",
       "189      1          1\n",
       "190      1          1\n",
       "191      0          0\n",
       "192      1          1\n",
       "193      1          1\n",
       "194      1          1\n",
       "195      0          1\n",
       "196      1          1\n",
       "197      1          1\n",
       "198      1          1\n",
       "199      1          1\n",
       "200      1          1\n",
       "201      1          1\n",
       "202      1          0\n",
       "203      1          1\n",
       "204      1          1\n",
       "205      1          1\n",
       "206      1          1\n",
       "207      1          1\n",
       "208      1          1\n",
       "209      1          1\n",
       "210      0          0\n",
       "211      1          1\n",
       "212      1          1\n",
       "213      1          1\n",
       "214      1          1\n",
       "\n",
       "[215 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_pred1 = nn1.predict(X1_test)\n",
    "\n",
    "pred = pd.DataFrame(list(zip(y1_test, nn_pred1)), columns=['y_act','y_nn_stan'])\n",
    "pred.index.name = 'Obs'\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Accuracy, confusion matrix, and classification reports are returned for the standardized design matirx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response vector has the following distribution: \n",
      "zeros: 0.14 \n",
      "ones: 0.83 \n",
      "twos: 0.02\n",
      "\n",
      "\n",
      "The accuracy of the Standardized Neural Network model is:  0.906976744186\n",
      "\n",
      "\n",
      "Standardized Neural Network Confusion Matrix: \n",
      "          Fail(0)  Pass(1)  Inc(2)\n",
      "Fail(0)       16        6       1\n",
      "Pass(1)       11      175       1\n",
      "Inc(2)         0        1       4\n",
      "\n",
      "\n",
      "Classification report for standardized design matrix:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.70      0.64        23\n",
      "          1       0.96      0.94      0.95       187\n",
      "          2       0.67      0.80      0.73         5\n",
      "\n",
      "avg / total       0.92      0.91      0.91       215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm_nn1 = pd.DataFrame(metrics.confusion_matrix(y1_test, nn_pred1), index = ['Fail(0)','Pass(1)','Inc(2)'],columns=['Fail(0)','Pass(1)','Inc(2)'])\n",
    "\n",
    "zero = 0\n",
    "one = 0\n",
    "two = 0\n",
    "for i in y1_train:\n",
    "    if i == 0:\n",
    "        zero += 1\n",
    "    elif i == 1:\n",
    "        one += 1\n",
    "    else:\n",
    "        two += 1\n",
    "num1 = round(zero/len(y1_train),2)\n",
    "num2 = round(one/len(y1_train),2)\n",
    "num3 = round(two/len(y1_train),2)\n",
    "print(\"The response vector has the following distribution: \\nzeros: %r \\nones: %r \\ntwos: %r\" % (num1,num2,num3))\n",
    "print(\"\\n\")\n",
    "\n",
    "print (\"The accuracy of the Standardized Neural Network model is: \", nn1.score(X1_test,y1_test))\n",
    "print (\"\\n\")\n",
    "\n",
    "print(\"Standardized Neural Network Confusion Matrix: \\n\", cm_nn1)\n",
    "print (\"\\n\")\n",
    "\n",
    "print(\"Classification report for standardized design matrix:\\n\", metrics.classification_report(y1_test,nn_pred1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
